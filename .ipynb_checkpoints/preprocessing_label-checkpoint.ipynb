{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmNbUQNSwIH2"
   },
   "source": [
    "# colab vm에 google drive mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSne4ncP7AlX"
   },
   "outputs": [],
   "source": [
    "### colab에 내 구글 drive mount하기\n",
    "\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "\n",
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "\n",
    "import getpass\n",
    "\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "\n",
    "vcode = getpass.getpass()\n",
    "\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DNkzVgX-HHa"
   },
   "outputs": [],
   "source": [
    "### mount할 drive 생성, 만든 계정 연결\n",
    "\n",
    "!mkdir -p /opt/bin/daehan/drive\n",
    "!google-drive-ocamlfuse /opt/bin/daehan/drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDzaArow0avp"
   },
   "outputs": [],
   "source": [
    "### drive mount\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/opt/bin/daehan/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gadc0cp_I8qZ"
   },
   "outputs": [],
   "source": [
    "### data upzip\n",
    "\n",
    "path_to_zip_file = '/opt/bin/daehan/data/train/[train] wav.zip'\n",
    "directory_to_extract_to = '/opt/bin/daehan/data/train'\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIDAeP1l4Q86"
   },
   "source": [
    "# 전처리에 필요한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Hh6ao35MWP5z"
   },
   "outputs": [],
   "source": [
    "# 전처리에 필요한 module\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 경로 설정\n",
    "\n",
    "BASE_DIR = '/opt/bin/daehan/'\n",
    "TRAIN_DIR = '/opt/bin/daehan/data/train/'\n",
    "LABEL_DIR = '/opt/bin/daehan/data/train_json/'\n",
    "TEST_DIR = '/opt/bin/daehan/data/test/'\n",
    "\n",
    "# 경로 내의 확장자별 파일 분리\n",
    "def file_filter(path):\n",
    "  input = sorted(os.listdir(path))\n",
    "  jjson = []\n",
    "  wav = []\n",
    "  flac = []\n",
    "\n",
    "  for n in input:\n",
    "    if '.flac' in n:\n",
    "      flac.append(n)\n",
    "    elif '.wav' in n:\n",
    "      wav.append(n)\n",
    "    else:\n",
    "      jjson.append(n)\n",
    "  return sorted(jjson), sorted(flac), sorted(wav)\n",
    "\n",
    "# json 파일의 메타데이터에서 파일명, 대본 라벨, 대본, 전사 추출\n",
    "def labeling(label):\n",
    "\n",
    "  '''\n",
    "  Filename: 음성데이터 파일 이름      >> list\n",
    "  SentenceID: 대본 라벨               >> list\n",
    "  SpeakerID: 화자 라벨                >> list\n",
    "  Utter_txt: 한글만 있는 대본         >> list\n",
    "  Gender: 화자 성별                    >> list\n",
    "  label >> sorted(os.listdir(path))결과\n",
    "  '''\n",
    "\n",
    "  Filename = []\n",
    "  SpeakerID = []\n",
    "  Utter_txt = []\n",
    "  SentenceID = []\n",
    "  Gender = []\n",
    "\n",
    "\n",
    "  for n in label:\n",
    "    if n[-4:] != 'json':\n",
    "      continue\n",
    "    with open(LABEL_DIR + n) as f:\n",
    "      label_info = json.load(f)\n",
    "\n",
    "      Filename.append(label_info['fileName'])\n",
    "      SentenceID.append(label_info['file_info']['sentenceID'])\n",
    "      SpeakerID.append(label_info['file_info']['speakerID'])\n",
    "      Utter_txt.append(label_info['transcription']['ReadingLabelText'])\n",
    "      Gender.append(label_info['basic_info']['gender'])\n",
    "\n",
    "  return Filename, SentenceID, SpeakerID, Utter_txt, Gender\n",
    "\n",
    "# 파일명 전사내용 trans.txt파일로 저장\n",
    "def make_file(x,SentenceID, SpeakerID, Utter_txt):\n",
    "  # label txt maker\n",
    "  text_file_path = '{}.trans.txt'.format(x+'_004')\n",
    "  with open(text_file_path, 'w', newline = '') as f:\n",
    "    for i in range(len(SentenceID)):\n",
    "      tmp = SpeakerID[i]+'_004_'+SentenceID[i]+' '+Utter_txt[i]+'\\n'\n",
    "      f.write(tmp)\n",
    "     \n",
    "\n",
    "  # 결과에 '\\n'와 중복값 제거 \n",
    "  new_text_content=''\n",
    "  with open(text_file_path,'r') as g:\n",
    "    \n",
    "    lines = g.readlines()\n",
    "    for i, l in enumerate(list(set(lines))):\n",
    "      if l == '\\n':\n",
    "        new_text_content += ''\n",
    "      elif l[:20] in new_text_content:\n",
    "        new_text_content += ''\n",
    "      else:\n",
    "        new_text_content += l\n",
    "\n",
    "  # 다시 저장\n",
    "  with open(text_file_path,'w') as f:\n",
    "    f.write(new_text_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wouchOqJ1AVd"
   },
   "source": [
    "# dataset.flac -> Speechdata/train_data_01/004/, /test_data_01/004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dtgbD406VJP"
   },
   "outputs": [],
   "source": [
    "\n",
    "# {'CN11RC001_CN0003_20210730', 'CN11RC001_CN0006_20210827', 'CN11RC001_CN0001_20210827'} : 누락 파일\n",
    "\n",
    "# DIR\n",
    "BASE_DIR = '/opt/bin/daehan/'\n",
    "TRAIN_DIR = '/opt/bin/daehan/data/train/'\n",
    "LABEL_DIR = '/opt/bin/daehan/data/train_json/'\n",
    "TEST_DIR = '/opt/bin/daehan/data/test/'\n",
    "SAVE_DIR = './model_saved/'\n",
    "\n",
    "# train,val 분할\n",
    "json_list = sorted(os.listdir(LABEL_DIR))\n",
    "json_list.remove('CN11RC001_CN0003_20210730_SW071.json')\n",
    "json_list.remove('CN11RC001_CN0001_20210827_SW076.json')\n",
    "json_list.remove('CN11RC001_CN0006_20210827_SW053.json')\n",
    "tr = os.listdir('/opt/bin/daehan/zeroth/s5/speechDATA/train_data_01/004/')\n",
    "te = os.listdir('/opt/bin/daehan/zeroth/s5/speechDATA/test_data_01/004')\n",
    "\n",
    "label_val = []\n",
    "\n",
    "for n in json_list:\n",
    "  for k in te:\n",
    "    if k in n:\n",
    "      label_val.append(n)\n",
    "label_train = list(set(json_list).difference(set(label_val)))\n",
    "\n",
    "_, SentenceID1, SpeakerID1, Utter_txt1, _ = labeling(label_train)\n",
    "_, SentenceID2, SpeakerID2, Utter_txt2, _ = labeling(label_val)\n",
    "\n",
    "# train 파일 flac변환 후 dataset에 저장\n",
    "\n",
    "os.chdir('/opt/bin/daehan/zeroth/s5/speechDATA/train_data_01/')\n",
    "\n",
    "train_zip = sorted(list(zip(SentenceID1, SpeakerID1, Utter_txt1)), key = lambda x: (x[1],x[0],x[2]))\n",
    "val_zip = sorted(list(zip(SentenceID2, SpeakerID2, Utter_txt2)), key = lambda x: (x[1],x[0],x[2]))\n",
    "\n",
    "for i in range(len(label_train)):\n",
    "  if os.path.exists('./004/'+SpeakerID1[i]+'/') == False:\n",
    "    os.mkdir('./004/'+SpeakerID1[i]+'/')\n",
    "\n",
    "  shutil.copy(LABEL_DIR + label_train[i], './004/'+SpeakerID1[i]+'/'+SpeakerID1[i]+'_004_'+SentenceID1[i]+'.json')\n",
    "  shutil.copy(TRAIN_DIR+flac_train[i],'./004/'+SpeakerID1[i]+'/'+SpeakerID1[i]+'_004_'+SentenceID1[i]+'.flac')\n",
    "\n",
    "# data/train_data_01 폴더에 text, utt2spk, wav.scp 업데이트 >> 필요 x\n",
    "\n",
    "  # with open('/opt/bin/daehan/zeroth/s5/data/train_data_01/text', 'a') as g:\n",
    "  #   g.write('{} {}\\n'.format(train_zip[i][1]+'_004_'+train_zip[i][0], train_zip[i][2]))\n",
    "  # with open('/opt/bin/daehan/zeroth/s5/data/train_data_01/utt2spk', 'a') as h:\n",
    "  #   h.write('{} {}\\n'.format(train_zip[i][1]+'_004_'+train_zip[i][0], train_zip[i][1]+'_004'))\n",
    "  # with open('/opt/bin/daehan/zeroth/s5/data/train_data_01/wav.scp', 'a') as j:\n",
    "  #   j.write('{0} flac -c -d -s ./speechDATA/train_data_01/004/{1}/{0}.flac |\\n'.format(train_zip[i][1]+'_004_'+train_zip[i][0], train_zip[i][1]))\n",
    "\n",
    "# data/test_data_01 ~~\n",
    "\n",
    "  # for i in range(len(label_val)):\n",
    "  # with open('/opt/bin/daehan/zeroth/s5/data/test_data_01/text', 'a') as g:\n",
    "  #     g.write('{} {}\\n'.format(val_zip[i][1]+'_004_'+val_zip[i][0], val_zip[i][2]))\n",
    "  # with open('/opt/bin/daehan/zeroth/s5/data/test_data_01/utt2spk', 'a') as h:\n",
    "  #   h.write('{} {}\\n'.format(val_zip[i][1]+'_004_'+val_zip[i][0], val_zip[i][1]+'_004'))\n",
    "  # with open('/opt/bin/daehan/zeroth/s5/data/test_data_01/wav.scp', 'a') as j:\n",
    "  #   j.write('{0} flac -c -d -s ./speechDATA/test_data_01/004/{1}/{0}.flac |\\n'.format(val_zip[i][1]+'_004_'+val_zip[i][0], val_zip[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BUWQDzfzaOP"
   },
   "source": [
    "# Audioinfo 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1K0o0FHXfBY8"
   },
   "outputs": [],
   "source": [
    "# flac 변화 실패한 data를 제외\n",
    "json_list = sorted(os.listdir(LABEL_DIR))\n",
    "json_list.remove('CN11RC001_CN0003_20210730_SW071.json')\n",
    "json_list.remove('CN11RC001_CN0001_20210827_SW076.json')\n",
    "json_list.remove('CN11RC001_CN0006_20210827_SW053.json')\n",
    "tr = os.listdir('/opt/bin/daehan/zeroth/s5/speechDATA/train_data_01/004/')\n",
    "te = os.listdir('/opt/bin/daehan/zeroth/s5/speechDATA/test_data_01/004')\n",
    "\n",
    "# train set에서 speakerID가 곂치지 않게 약 10% val set을  분할\n",
    "\n",
    "label_val = []\n",
    "\n",
    "for n in json_list:\n",
    "  for k in te:\n",
    "    if k in n:\n",
    "      label_val.append(n)\n",
    "label_train = list(set(json_list).difference(set(label_val)))\n",
    "\n",
    "# metadata에서 필요한 data 추출\n",
    "_, _, SpeakerID1, _, Gender1 = labeling(label_train)\n",
    "_, _, SpeakerID2, _, Gender2 = labeling(label_val)\n",
    "\n",
    "train_lst = list(set(zip(SpeakerID1,Gender1)))\n",
    "val_lst = list(set(zip(SpeakerID2, Gender2)))\n",
    "# train_set\n",
    "with open('/opt/bin/daehan/zeroth/s5/speechDATA/AUDIO_INFO', 'a') as f:\n",
    "    for i in range(len(train_lst)):\n",
    "      f.write('{}|{}|{}|004|train_data_01\\n'.format(train_lst[i][0],train_lst[i][0],train_lst[i][1].lower()))\n",
    "# val_set\n",
    "with open('/opt/bin/daehan/zeroth/s5/speechDATA/AUDIO_INFO', 'a') as f:\n",
    "    for i in range(len(val_lst)):\n",
    "      f.write('{}|{}|{}|004|test_data_01\\n'.format(val_lst[i][0], val_lst[i][0], val_lst[i][1].lower()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQrsndOAyiCe"
   },
   "source": [
    "# speakerID별 trans.txt 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bE2SrboiyFXj"
   },
   "outputs": [],
   "source": [
    "# DIR 설정\n",
    "BASE_DIR = '/opt/bin/daehan/'\n",
    "TRAIN_DIR = '/opt/bin/daehan/data/train/'\n",
    "LABEL_DIR = '/opt/bin/daehan/data/train_json/'\n",
    "TEST_DIR = '/opt/bin/daehan/data/test/'\n",
    "\n",
    "# _ , flac = file_filter(TRAIN_DIR) \n",
    "json_list = sorted(os.listdir(LABEL_DIR))\n",
    "json_list.remove('CN11RC001_CN0003_20210730_SW071.json')\n",
    "json_list.remove('CN11RC001_CN0001_20210827_SW076.json')\n",
    "json_list.remove('CN11RC001_CN0006_20210827_SW053.json')\n",
    "tr = os.listdir('/opt/bin/daehan/zeroth/s5/speechDATA/train_data_01/004/')\n",
    "te = os.listdir('/opt/bin/daehan/zeroth/s5/speechDATA/test_data_01/004')\n",
    "\n",
    "# train set에서 speakerID가 곂치지 않게 약 10% val set을  분할\n",
    "\n",
    "label_val = []\n",
    "\n",
    "for n in json_list:\n",
    "    for k in te:\n",
    "        if k in n:\n",
    "            label_val.append(n)\n",
    "label_train = list(set(json_list).difference(set(label_val)))\n",
    "\n",
    "# trainset과 val. set에 해당하는 json파일에서 필요한 요소 추출, 정렬\n",
    "_, SentenceID1, SpeakerID1, Utter_txt1, _ = labeling(label_train)\n",
    "_, SentenceID2, SpeakerID2, Utter_txt2, _ = labeling(label_val)\n",
    "\n",
    "train_zip = sorted(list(zip(SentenceID1, SpeakerID1, Utter_txt1)), key = lambda x: (x[1],x[0],x[2]))\n",
    "val_zip = sorted(list(zip(SentenceID2, SpeakerID2, Utter_txt2)), key = lambda x: (x[1],x[0],x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1738,
     "status": "ok",
     "timestamp": 1639390666681,
     "user": {
      "displayName": "이민수",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14114424375921079673"
     },
     "user_tz": -540
    },
    "id": "u4piDIM0QNEe",
    "outputId": "0cf4ba69-839e-417a-fba0-561dd4399797"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 5/138 [00:00<00:02, 45.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 167 167\n",
      "265 265 265\n",
      "861 861 861\n",
      "743 743 743\n",
      "817 817 817\n",
      "542 542 542\n",
      "257 257 257\n",
      "382 382 382\n",
      "216 216 216\n",
      "562 562 562\n",
      "294 294 294\n",
      "374 374 374\n",
      "360 360 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 20/138 [00:00<00:01, 60.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973 973 973\n",
      "210 210 210\n",
      "618 618 618\n",
      "463 463 463\n",
      "8 8 8\n",
      "920 920 920\n",
      "208 208 208\n",
      "902 902 902\n",
      "370 370 370\n",
      "821 821 821\n",
      "651 651 651\n",
      "872 872 872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 33/138 [00:00<00:01, 53.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577 577 577\n",
      "2 2 2\n",
      "921 921 921\n",
      "761 761 761\n",
      "918 918 918\n",
      "844 844 844\n",
      "308 308 308\n",
      "999 999 999\n",
      "368 368 368\n",
      "237 237 237\n",
      "269 269 269\n",
      "944 944 944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 53/138 [00:00<00:01, 75.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602 602 602\n",
      "670 670 670\n",
      "64 64 64\n",
      "359 359 359\n",
      "182 182 182\n",
      "135 135 135\n",
      "174 174 174\n",
      "5 5 5\n",
      "360 360 360\n",
      "360 360 360\n",
      "320 320 320\n",
      "107 107 107\n",
      "179 179 179\n",
      "87 87 87\n",
      "320 320 320\n",
      "373 373 373\n",
      "243 243 243\n",
      "375 375 375\n",
      "389 389 389\n",
      "117 117 117\n",
      "191 191 191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 77/138 [00:01<00:00, 94.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 83 83\n",
      "261 261 261\n",
      "4 4 4\n",
      "350 350 350\n",
      "202 202 202\n",
      "343 343 343\n",
      "296 296 296\n",
      "374 374 374\n",
      "5 5 5\n",
      "163 163 163\n",
      "21 21 21\n",
      "77 77 77\n",
      "73 73 73\n",
      "354 354 354\n",
      "192 192 192\n",
      "314 314 314\n",
      "175 175 175\n",
      "166 166 166\n",
      "248 248 248\n",
      "309 309 309\n",
      "278 278 278\n",
      "327 327 327\n",
      "141 141 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 100/138 [00:01<00:00, 102.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 112 112\n",
      "68 68 68\n",
      "173 173 173\n",
      "357 357 357\n",
      "161 161 161\n",
      "11 11 11\n",
      "332 332 332\n",
      "93 93 93\n",
      "123 123 123\n",
      "7 7 7\n",
      "344 344 344\n",
      "353 353 353\n",
      "355 355 355\n",
      "122 122 122\n",
      "110 110 110\n",
      "365 365 365\n",
      "149 149 149\n",
      "71 71 71\n",
      "396 396 396\n",
      "175 175 175\n",
      "325 325 325\n",
      "326 326 326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 124/138 [00:01<00:00, 106.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1\n",
      "9 9 9\n",
      "1 1 1\n",
      "168 168 168\n",
      "286 286 286\n",
      "376 376 376\n",
      "256 256 256\n",
      "183 183 183\n",
      "17 17 17\n",
      "158 158 158\n",
      "148 148 148\n",
      "330 330 330\n",
      "186 186 186\n",
      "150 150 150\n",
      "51 51 51\n",
      "318 318 318\n",
      "3 3 3\n",
      "167 167 167\n",
      "1 1 1\n",
      "272 272 272\n",
      "245 245 245\n",
      "39 39 39\n",
      "74 74 74\n",
      "170 170 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138/138 [00:01<00:00, 88.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 55 55\n",
      "194 194 194\n",
      "76 76 76\n",
      "173 173 173\n",
      "181 181 181\n",
      "94 94 94\n",
      "25 25 25\n",
      "285 285 285\n",
      "173 173 173\n",
      "151 151 151\n",
      "184 184 184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train trans.txt생성\n",
    "for x in tqdm(sorted(os.listdir('/opt/bin/daehan/zeroth/s5/speechDATA/train_data_01/004/'))):\n",
    "  temp = []\n",
    "  LABEL_DIR = '/opt/bin/daehan/zeroth/s5/speechDATA/train_data_01/004/'+ x +'/'\n",
    "  os.chdir(LABEL_DIR)\n",
    "  for i in train_zip:\n",
    "    if x == i[1]:\n",
    "      temp.append(i)\n",
    "    else:\n",
    "      continue\n",
    "  SentenceID, SpeakerID, Utter_txt = zip(*temp)\n",
    "  print(len(SentenceID),len(SpeakerID),len(Utter_txt))\n",
    "  make_file(x, SentenceID, SpeakerID, Utter_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1007,
     "status": "ok",
     "timestamp": 1639375475820,
     "user": {
      "displayName": "이민수",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14114424375921079673"
     },
     "user_tz": -540
    },
    "id": "PHDQQ9sD1Ka1",
    "outputId": "b897aaa4-95f3-4868-c058-8c756c9a910e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 167 167\n",
      "265 265 265\n",
      "861 861 861\n",
      "743 743 743\n",
      "817 817 817\n",
      "542 542 542\n",
      "257 257 257\n",
      "382 382 382\n",
      "216 216 216\n",
      "562 562 562\n",
      "294 294 294\n",
      "374 374 374\n",
      "360 360 360\n",
      "973 973 973\n",
      "210 210 210\n",
      "618 618 618\n",
      "463 463 463\n",
      "8 8 8\n",
      "920 920 920\n",
      "208 208 208\n",
      "902 902 902\n",
      "370 370 370\n",
      "821 821 821\n",
      "651 651 651\n",
      "872 872 872\n",
      "577 577 577\n",
      "2 2 2\n",
      "921 921 921\n",
      "761 761 761\n",
      "918 918 918\n",
      "844 844 844\n",
      "308 308 308\n",
      "999 999 999\n",
      "368 368 368\n",
      "237 237 237\n",
      "269 269 269\n",
      "944 944 944\n",
      "602 602 602\n",
      "670 670 670\n",
      "64 64 64\n",
      "359 359 359\n",
      "182 182 182\n",
      "135 135 135\n",
      "174 174 174\n",
      "5 5 5\n",
      "360 360 360\n",
      "360 360 360\n",
      "320 320 320\n",
      "107 107 107\n",
      "179 179 179\n",
      "87 87 87\n",
      "320 320 320\n",
      "373 373 373\n",
      "243 243 243\n",
      "375 375 375\n",
      "389 389 389\n",
      "117 117 117\n",
      "191 191 191\n",
      "83 83 83\n",
      "261 261 261\n",
      "4 4 4\n",
      "350 350 350\n",
      "202 202 202\n",
      "343 343 343\n",
      "296 296 296\n",
      "374 374 374\n",
      "5 5 5\n",
      "163 163 163\n",
      "21 21 21\n",
      "77 77 77\n",
      "73 73 73\n",
      "354 354 354\n",
      "192 192 192\n",
      "314 314 314\n",
      "175 175 175\n",
      "166 166 166\n",
      "248 248 248\n",
      "309 309 309\n",
      "278 278 278\n",
      "327 327 327\n",
      "141 141 141\n",
      "112 112 112\n",
      "68 68 68\n",
      "173 173 173\n",
      "357 357 357\n",
      "161 161 161\n",
      "11 11 11\n",
      "332 332 332\n",
      "93 93 93\n",
      "123 123 123\n",
      "7 7 7\n",
      "344 344 344\n",
      "353 353 353\n",
      "355 355 355\n",
      "122 122 122\n",
      "110 110 110\n",
      "365 365 365\n",
      "149 149 149\n",
      "71 71 71\n",
      "396 396 396\n",
      "175 175 175\n",
      "325 325 325\n",
      "326 326 326\n",
      "1 1 1\n",
      "9 9 9\n",
      "1 1 1\n",
      "168 168 168\n",
      "286 286 286\n",
      "376 376 376\n",
      "256 256 256\n",
      "183 183 183\n",
      "17 17 17\n",
      "158 158 158\n",
      "148 148 148\n",
      "330 330 330\n",
      "186 186 186\n",
      "150 150 150\n",
      "51 51 51\n",
      "318 318 318\n",
      "3 3 3\n",
      "167 167 167\n",
      "1 1 1\n",
      "272 272 272\n",
      "245 245 245\n",
      "39 39 39\n",
      "74 74 74\n",
      "170 170 170\n",
      "55 55 55\n",
      "194 194 194\n",
      "76 76 76\n",
      "173 173 173\n",
      "181 181 181\n",
      "94 94 94\n",
      "25 25 25\n",
      "285 285 285\n",
      "173 173 173\n",
      "151 151 151\n",
      "184 184 184\n"
     ]
    }
   ],
   "source": [
    "# test trans.txt 생성\n",
    "for x in sorted(os.listdir('/opt/bin/daehan/zeroth/s5/speechDATA/test_data_01/004/')):\n",
    "  temp = []\n",
    "  LABEL_DIR = '/opt/bin/daehan/zeroth/s5/speechDATA/test_data_01/004/'+ x +'/'\n",
    "  os.chdir(LABEL_DIR)\n",
    "  for i in val_zip:\n",
    "    if x == i[1]:\n",
    "      temp.append(i)\n",
    "    else:\n",
    "      continue\n",
    "  SentenceID, SpeakerID, Utter_txt = zip(*temp)\n",
    "  # print(len(SentenceID),len(SpeakerID),len(Utter_txt))\n",
    "  make_file(x,SentenceID, SpeakerID, Utter_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVp3NYRI3EhL"
   },
   "source": [
    "# Debug용 메모장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFfbz6UzAiXP"
   },
   "outputs": [],
   "source": [
    "# 성별 확인\n",
    "a = []\n",
    "with open('/opt/bin/daehan/zeroth/s5/speechDATA/AUDIO_INFO', 'r') as f:\n",
    "  lines = f.readlines()\n",
    "  for line in lines:\n",
    "    line = line.strip('\\n').split('|')\n",
    "    a.append(line) \n",
    "gender = [i[2] for i in a]\n",
    "foo = [i for i in sex if i != 'f' and i !='m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSf1SjAJIydr"
   },
   "outputs": [],
   "source": [
    "train_dir = set(os.listdir('/opt/bin/daehan/zeroth/s5/speechDATA/train_data_01/004/'))\n",
    "test_dir = set(os.listdir('/opt/bin/daehan/zeroth/s5/speechDATA/test_data_01/004/'))\n",
    "same = list(train_dir.intersection(test_dir))\n",
    "\n",
    "for n in same:\n",
    "  shutil.rmtree('/opt/bin/daehan/zeroth/s5/speechDATA/train_data_01/004/'+n+'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9vF1idyfzUf"
   },
   "outputs": [],
   "source": [
    "CN = [n for n in sorted(label_train) if 'CN0001' in n]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMuzKwAexT5vVugJlvM0l57",
   "collapsed_sections": [
    "lmNbUQNSwIH2",
    "gIDAeP1l4Q86",
    "wouchOqJ1AVd",
    "3BUWQDzfzaOP",
    "bQrsndOAyiCe",
    "MVp3NYRI3EhL"
   ],
   "name": "preprocessing_label.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
